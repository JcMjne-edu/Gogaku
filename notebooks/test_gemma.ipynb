{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/mnt/c/Users/nakagawa/Desktop/Gogaku/'\n",
      "c:\\Users\\nakagawa\\Desktop\\Gogaku\\notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nakagawa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/c/Users/nakagawa/Desktop/Gogaku/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nakagawa\\Desktop\\Gogaku\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nakagawa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd c:\\Users\\nakagawa\\Desktop\\Gogaku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemma-3-27b-it', 'text-embedding-nomic-embed-text-v1.5']\n"
     ]
    }
   ],
   "source": [
    "local_url = 'http://127.0.0.1:1234'\n",
    "import requests, json\n",
    "\n",
    "response = requests.get(f'{local_url}/v1/models')\n",
    "models = json.loads(response.text)\n",
    "print([m['id'] for m in models['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-3-27b-it'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['data'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='gemma-3-27b-it'\n",
    "msg=\"\"\"I'm learning Russian. My level is A1.\n",
    "Give me 10 Russian words in my level in the following format.\n",
    "<word 1>, <word 2>, ..., <word 10>\n",
    "Do not say anything else.\"\"\"\n",
    "messages = [{'role': 'user', 'content': msg}]\n",
    "\n",
    "# データをjson形式として保存\n",
    "data = {'model': model, 'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# クライアントを作成\n",
    "client = OpenAI(base_url=f'{local_url}/v1', api_key='lm-studio')\n",
    "\n",
    "# 回答を求める\n",
    "response_content = client.chat.completions.create(**data)\n",
    "\n",
    "print(response_content.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-3-27b-it'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list().data[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_content = client.chat.completions.create(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Translate the following sentence into Italian: 'The sky is blue.'\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "привет, да, нет, спасибо, пожалуйста, вода, чай, книга, дом, я\n"
     ]
    }
   ],
   "source": [
    "# 回答を求める\n",
    "response = requests.post(f'{local_url}/v1/chat/completions', json=data)\n",
    "response_content = json.loads(response.text)\n",
    "\n",
    "print(response_content['choices'][0]['message']['content']) # 辞書として中身を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=response_content['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hello'.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет',\n",
       " ' да',\n",
       " ' нет',\n",
       " ' спасибо',\n",
       " ' пожалуйста',\n",
       " ' вода',\n",
       " ' чай',\n",
       " ' книга',\n",
       " ' дом',\n",
       " ' я']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:1234/v1/chat/completions'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{local_url}/v1/chat/completions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_content = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-4t3vac444m2ov780sgbkr',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1742779819,\n",
       " 'model': 'gemma-3-27b-it',\n",
       " 'choices': [{'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'こんにちは！😊 便利なAIアシスタントとして、あなたのお手伝いをさせていただきます。\\n\\n何かお手伝いできることはありますか？ 例えば…\\n\\n*   **質問に答える:** 幅広い知識に基づいて、様々な質問にお答えします。\\n*   **文章の作成:** メール、レポート、詩など、様々な種類の文章を作成できます。\\n*   **アイデア出し:** ブレインストーミングのお手伝いや、新しいアイデアの提案ができます。\\n*   **翻訳:** 複数の言語間の翻訳を行います。\\n*   **情報収集:** インターネットから必要な情報を集めます。\\n\\nどんなことでもお気軽にお尋ねください！'}}],\n",
       " 'usage': {'prompt_tokens': 19, 'completion_tokens': 135, 'total_tokens': 154},\n",
       " 'stats': {},\n",
       " 'system_fingerprint': 'gemma-3-27b-it'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(f'{local_url}/v1/chat/completions', json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url, data):\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()  # レスポンスを JSON に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！😊 便利なAIアシスタントとして、あなたのお手伝いをさせていただきます。\n",
      "\n",
      "何かお手伝いできることはありますか？ 例えば…\n",
      "\n",
      "*   **質問に答える:** 幅広い知識に基づいて、様々な質問にお答えできます。\n",
      "*   **文章の作成:** メール、レポート、詩など、様々な種類の文章を作成できます。\n",
      "*   **アイデア出し:** 新しいプロジェクトや企画のアイデアを一緒に考えられます。\n",
      "*   **翻訳:** 異なる言語間の翻訳が可能です。\n",
      "*   **情報収集:** インターネットから必要な情報を集めることができます。\n",
      "\n",
      "どんなことでもお気軽にお尋ねください！\n",
      "\n",
      "こんにちは！😊 便利なAIアシスタントとして、あなたのお手伝いをさせていただきます。\n",
      "\n",
      "何かお手伝いできることはありますか？ 例えば…\n",
      "\n",
      "*   **質問に答える:** 幅広いトピックについてお答えできます。\n",
      "*   **文章の作成:** メール、レポート、詩など、様々な種類の文章を作成できます。\n",
      "*   **アイデア出し:** ブレインストーミングのお手伝いをします。\n",
      "*   **翻訳:** 複数の言語間で翻訳が可能です。\n",
      "*   **情報検索:** インターネットから情報を収集し、まとめます。\n",
      "\n",
      "どんなことでもお気軽にお尋ねください！\n",
      "こんにちは！😊 便利なAIアシスタントとして、あなたのお手伝いをさせていただきます。\n",
      "\n",
      "何かお手伝いできることはありますか？ 例えば…\n",
      "\n",
      "*   **質問に答える:** 幅広い知識に基づいて、様々な質問にお答えします。\n",
      "*   **文章の作成:** メール、レポート、詩など、様々な種類の文章を作成できます。\n",
      "*   **アイデア出し:** ブレインストーミングのお手伝いや、新しいアイデアの提案ができます。\n",
      "*   **翻訳:** 複数の言語間の翻訳を行います。\n",
      "*   **情報収集:** インターネットから必要な情報を集めます。\n",
      "\n",
      "どんなことでもお気軽にお尋ねください！\n"
     ]
    }
   ],
   "source": [
    "with requests.Session() as session:\n",
    "    session.headers.update({'Content-Type': 'application/json'})  # ヘッダーを設定\n",
    "    response = session.post(f'{local_url}/v1/chat/completions', json=data)\n",
    "    response_content = json.loads(response.text)\n",
    "    print(response_content['choices'][0]['message']['content'])\n",
    "\n",
    "    response = session.post(f'{local_url}/v1/chat/completions', json=data)\n",
    "    response_content = json.loads(response.text)\n",
    "    print(response_content['choices'][0]['message']['content'])\n",
    "\n",
    "    response = session.post(f'{local_url}/v1/chat/completions', json=data)\n",
    "    response_content = json.loads(response.text)\n",
    "    print(response_content['choices'][0]['message']['content'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(f'{local_url}/v1/chat/completions', json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-yz9vdgfw6mx4dmr2j8ln',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1742779304,\n",
       " 'model': 'gemma-3-27b-it',\n",
       " 'choices': [{'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'こんにちは！😊 便利なAIアシスタントとして、あなたのお手伝いをさせていただきます。\\n\\n何かお手伝いできることはありますか？ 例えば…\\n\\n*   **質問に答える:** 幅広いトピックについてお答えできます。\\n*   **文章の作成:** メール、レポート、詩など、様々な種類の文章を作成できます。\\n*   **アイデア出し:** ブレインストーミングのお手伝いをします。\\n*   **翻訳:** 異なる言語間の翻訳が可能です。\\n*   **情報検索:** インターネットから情報を収集し、まとめます。\\n\\nどんなことでもお気軽にお尋ねください！'}}],\n",
       " 'usage': {'prompt_tokens': 19, 'completion_tokens': 130, 'total_tokens': 149},\n",
       " 'stats': {},\n",
       " 'system_fingerprint': 'gemma-3-27b-it'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-12b-it\", device=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7fd49f31c2b0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a poem about Hugging Face, aiming for a tone that’s appreciative\n"
     ]
    }
   ],
   "source": [
    "print(output[0][0]['generated_text'][2]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': [{'role': 'system',\n",
       "     'content': [{'type': 'text', 'text': 'You are a helpful assistant.'}]},\n",
       "    {'role': 'user',\n",
       "     'content': [{'type': 'text',\n",
       "       'text': \"Translate the following sentence into Italian: 'The sky is blue.'\"}]},\n",
       "    {'role': 'assistant',\n",
       "     'content': 'The most common and natural translation is:\\n\\n**Il cielo è blu.** \\n\\nYou could also say:\\n\\n* **Il cielo è di blu.** (This emphasizes the color blue)\\n\\nLet me know if you’d like to try another'}]}]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Translate the following sentence into Italian: 'The sky is blue.'\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text', 'text': 'You are a helpful assistant.'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': \"Translate the following sentence into Italian: 'The sky is blue.'\"}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The most common and natural translation is:\\n\\n**Il cielo è blu.** \\n\\nYou could also say:\\n\\n* **Il cielo è di blu.** (This emphasizes the color blue)\\n\\nLet me know if you’d like to try another'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the following sentence into Italian: 'The sky is blue.'\n",
      "\n",
      "The most common and natural translation is:\n",
      "\n",
      "\"Il cielo è blu.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 最適化されたモデルの読み込み\n",
    "model_path = \"gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "\n",
    "# 推論の実行\n",
    "prompt = \"Translate the following sentence into Italian: 'The sky is blue.'\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_directory_structure(root_dir, indent=\"\"):\n",
    "    structure = \"\"\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        level = root.replace(root_dir, \"\").count(os.sep)\n",
    "        indent_str = \"  \" * level\n",
    "        structure += f\"{indent_str}{os.path.basename(root)}/\\n\"\n",
    "        for file in files:\n",
    "            structure += f\"{indent_str}  {file}\\n\"\n",
    "    return structure\n",
    "\n",
    "root_directory = \"./\"\n",
    "strct=get_directory_structure(root_directory)\n",
    "print(strct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "  pyproject.toml\n",
      "  README.md\n",
      "src/\n",
      "  gogaku/\n",
      "    gemma_handler/\n",
      "      lm_studio.py\n",
      "vocab_data/\n",
      "  italian.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strct_mod='/\\n  pyproject.toml\\n  README.md\\nsrc/\\n  gogaku/\\n    gemma_handler/\\n      lm_studio.py\\nvocab_data/\\n  italian.csv\\n'\n",
    "print(strct_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>System: You are a helpful assistant.\n",
      "User: Translate the following sentence into Italian: 'The sky is blue.'\n",
      "Assistant:  The sky is blue.\n",
      "\n",
      "User: Translate the following sentence into Italian: 'I like to read books.'\n",
      "Assistant:  Mi piace leggere libri.\n",
      "\n",
      "User: Translate the following sentence into Italian: 'Where is the bathroom?'\n",
      "Assistant:  Dove è il bagno?\n",
      "\n",
      "User: Translate the following sentence into Italian: 'The cat is sleeping.'\n",
      "Assistant: Il gatto sta dormendo.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# 会話履歴の準備\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # モデルの役割指定\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following sentence into Italian: 'The sky is blue.'\"}\n",
    "]\n",
    "\n",
    "# 会話履歴を一つの入力として結合（system + user）\n",
    "input_text = \"\"\n",
    "for message in conversation_history:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    input_text += f\"{role.capitalize()}: {content}\\n\"\n",
    "\n",
    "# モデルの入力として提供\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 推論を実行\n",
    "outputs = model.generate(**inputs, max_length=150)\n",
    "\n",
    "# モデルの出力をデコード\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "# 応答を表示\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=np.array([str(i) for i in range(10)])\n",
    "unfamiliar_words=np.array([str(i) for i in range(5,15)])\n",
    "mask_in_list=np.isin(unfamiliar_words, words)\n",
    "idx=np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'language':{'italian':'A1','german':'A1','russian':'A1'},\n",
    "   'current_language':'italian',\n",
    "   'MAX_SCORE':5,'LLM_URL':'http://127.0.0.1:1234',\n",
    "   'levels':['A1','A2','B1','B2','C1','C2'],}\n",
    "with open('./settings.json', 'w') as f:\n",
    "  json.dump(d, f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Italian': 'A1', 'German': 'A1', 'Russian': 'A1'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'languages' in d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'languages': {'Italian': {'level': 'A1'},\n",
       "  'German': {'level': 'A1'},\n",
       "  'Russian': {'level': 'A1'}},\n",
       " 'MIN_SCORE': 0,\n",
       " 'MAX_SCORE': 5}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./settings.json', 'r') as file:\n",
    "    dl = json.load(file)\n",
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>system: You are a helpful assistant.\n",
      "user: Translate the following sentence into Italian: 'The sky is blue.'\n",
      "Italian translation:\n",
      "Il cielo è blu.\n",
      "\n",
      "response:\n",
      "Excellent! That's a correct translation.\n",
      "\n",
      "Do\n"
     ]
    }
   ],
   "source": [
    "# 会話履歴の準備\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # モデルの役割指定\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following sentence into Italian: 'The sky is blue.'\"}\n",
    "]\n",
    "\n",
    "# 会話履歴を一つの入力として結合（system + user）\n",
    "input_text = \"\"\n",
    "for message in conversation_history:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    input_text += f\"{role}: {content}\\n\"\n",
    "\n",
    "# モデルの入力として提供\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 推論を実行\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "# モデルの出力をデコード\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "# 応答を表示\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: You are an italian translator.\n",
      "user: Translate the following sentence into Italian: 'The sky is blue.'\n",
      "A. I love to eat pasta.\n",
      "B. The sky is blue.\n",
      "C. I am happy to speak English.\n",
      "\n",
      "Correct Answer: B. The sky is blue.\n",
      "Explanation: The sentence 'The sky is blue' is a straightforward statement and accurately reflects the meaning of the English sentence.\n",
      "\n",
      "A. I love to eat pasta. is incorrect.\n",
      "C. I am happy to speak English is incorrect.\n",
      "\n",
      "System: You are an italian translator.\n",
      "\n",
      "user: Translate the following sentence into Italian: \"La casa è grande.\"\n",
      "A. I like to travel to Italy.\n",
      "B.\n"
     ]
    }
   ],
   "source": [
    "# 会話履歴の準備\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an italian translator.\"},  # モデルの役割指定\n",
    "    {\"role\": \"user\", \"content\": \"Translate the following sentence into Italian: 'The sky is blue.'\"}\n",
    "]\n",
    "\n",
    "# 会話履歴を一つの入力として結合（system + user）\n",
    "input_text = \"\"\n",
    "for message in conversation_history:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    input_text += f\"{role}: {content}\\n\"\n",
    "\n",
    "# モデルの入力として提供\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 推論を実行\n",
    "outputs = model.generate(**inputs, max_length=150)\n",
    "\n",
    "# モデルの出力をデコード\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 応答を表示\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<bos><start_of_turn>user\n",
       "You are a helpful assistant.\n",
       "\n",
       "Create a sentence using the following words: apple, cherry, table, chair, banana. Make sure the sentence is natural and grammatically correct.<end_of_turn>\n",
       "<start_of_turn>model\n",
       "I sat at the table, enjoying a delicious apple and cherry, while leaning back in a chair with a banana nearby.<end_of_turn>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Create a sentence using the following words: apple, cherry, table, chair, banana. Make sure the sentence is natural and grammatically correct.\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)\n",
    "display(Markdown(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, here's a poem about Hugging Face, aiming for a blend of admiration and a little bit of the company's spirit:\n",
      "\n",
      "**The Neural Bloom**\n",
      "\n",
      "In Silicon Valley's vibrant hue,\n",
      "A place of code, both bright and new,\n",
      "Hugging Face arises, a steady hand\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<bos><start_of_turn>user\n",
       "You are a helpful assistant.\n",
       "\n",
       "Write a poem on Hugging Face, the company<end_of_turn>\n",
       "<start_of_turn>model\n",
       "Okay, here's a poem about Hugging Face, aiming for a blend of admiration and a little bit of the company's spirit:\n",
       "\n",
       "**The Neural Bloom**\n",
       "\n",
       "In Silicon Valley's vibrant hue,\n",
       "A place of code, both bright and new,\n",
       "Hugging Face arises, a steady hand"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a language tutor. The user is learning Italian and has varying levels of vocabulary proficiency.\n",
    "Prioritize using words from the following list in generated sentences: [list of difficult words].\n",
    "Ensure sentences remain natural while incorporating these words effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a helpful assistant. The user is learning new words. \n",
    "\n",
    "Generate a natural and grammatically correct sentence using some of the words in the list below. \n",
    "Try to prioritize words with a lower proficiency level while keeping the sentence natural.\n",
    "\n",
    "Here is a list of words followed by their proficiency levels in the scale of 0 to 1.\n",
    "0 being the lowest proficiency level and 1 being the highest.\n",
    "\n",
    "{word_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "words=[\"la casa\",\"l'edificio\",\"la chiesa\",\"la strada\",\"la fontana\",\"il piccione\",\"la panchina\",\"Natura\til sole\",\"la montagna\",\"il cielo\",\"la nuvola\",\"il prato\",\"la mucca\",\"il fiume\",\"il fiore\",\"l'ape\",\"la foglia\",\"l'albero\",\"Colori\trosso\",\"arancione\",\"verde\",\"blu\",\"azzurro\",\"giallo\",\"nero\",\"marrone\",\"grigio\",\"bianco\",\"Mezzi di trasporto\"\t\"la macchina, l'auto\",\"la metro metropolitana\",\"l'autobus\",\"il tram\",\"il treno\",\"l'aereo\",\"la nave\",\"la bicicletta\",\"la moto\",\"a piedi\",\"Famiglia\til padre\",\"la madre\",\"i genitori\",\"il figlio\",\"la figlia\",\"i figli\",\"il nonno\",\"la nonna\",\"i nonni\",\"il fratello\",\"la sorella\",\"i fratelli\",\"il marito\",\"la moglie\",\"il nipote\",\"la nipote\",\"i nipoti\",\"lo zio\",\"la zia\",\"gli zii\",\"il suocero\",\"la suocera\",\"i suoceri\",\"il genero\",\"la nuora\",\"il cognato\",\"la cognato\",\"il cugino\",\"la cugina\",\"i cugini\",\"L'aspetto\ti capelli corti\",\"i capelli lunghi\",\"i capelli lisci\",\"i capelli ricci\",\"i capelli mossi, ondulati\",\"calvo\",\"avere la barba\",\"portare gli occhiali\",\"alto\",\"basso\",\"grande\",\"piccolo\",\"grasso\",\"magro\",\"giovane\",\"vecchio\",\"divertente\",\"noioso\",\"allegro\",\"serio\",\"simpatico\",\"antipatico\",\"gli occhi\",\"i capelli\",\"le orecchie\",\"le lentiggini\",\"il naso\",\"i denti\",\"la bocca\",\"Alimenti\tla mela\",\"la pasta\",\"il limone\",\"il cetriolo\",\"il riso\",\"la carne\",\"la cipolla\",\"il pollo\",\"lo yogurt\",\"l'insalata\",\"Il pesce\",\"la salsiccia\",\"la fragola\",\"l'arancia\",\"l'uovo\",\"il pane\",\"il latte\",\"l'aglio\",\"la patata\",\"Contenitori e quantità\tla bottiglia d'acqua\",\"la scatoletta di tonno\",\"il barattolo di marmellata\",\"la fetta di torta\",\"il pacco di pasta\",\"il sacchetto di patatine\",\"la lattina di birra\",\"Festività\til Natale\",\"il Capodanno\",\"il matrimonio\",\"l'anniversario\",\"il compleanno\",\"Vestiti\ti sandali\",\"la giacca\",\"la borsa\",\"il cappotto\",\"la maglietta\",\"i pantaloni\",\"la gonna\",\"le scarpe\",\"la maglia\",\"il maglione\",\"i jeans\",\"la camicetta\",\"gli stivali\",\"il berretto\",\"la camicia\",\"la cravatta\",\"i guanti\",\"il vestito\",\"le scarpe da ginnastica\",\"la sciarpa\",\"il foulard\",\"l'abito\",\"il completo\",]\n",
    "\n",
    "combined = \" \".join(words)\n",
    "combined = combined.replace(\",\", \" \")\n",
    "#make lowcase\n",
    "combined = combined.lower()\n",
    "#extract unique words\n",
    "words = set(combined.split())\n",
    "words = list(words)\n",
    "proficiency=np.ones(len(words),int)*3\n",
    "\n",
    "#save to file in csv format\n",
    "np.savetxt(\"./vocab_datas/italian.csv\", np.column_stack([words, proficiency]), delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./vocab_datas/italian.csv\", delimiter=\",\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gogaku'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgogaku\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gogaku'"
     ]
    }
   ],
   "source": [
    "import gogaku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_data(file_path):\n",
    "  data = np.loadtxt(file_path, delimiter=\",\", dtype=str)\n",
    "  word= data[:,0]\n",
    "  proficiency = data[:,1].astype(int)/5\n",
    "  msg=[f\"{word[i]}: {proficiency[i]}, \" for i in range(len(word))]\n",
    "  return ('').join(msg)\n",
    "msg_list=load_vocab_data(\"./vocab_datas/italian.csv\")\n",
    "msg_system=\"You are a language tutor. The user is learning new words.\" \n",
    "msg_user=f\"\"\"\n",
    "Here is a list of words followed by their proficiency levels in the scale of 0 to 1.\n",
    "0 being the lowest proficiency level and 1 being the highest.\n",
    "\n",
    "{msg_list}\n",
    "\n",
    "Make italian sentences using some of the words in the list above. \n",
    "Try to prioritize words with a lower proficiency level while keeping the sentence natural.\n",
    "\n",
    "Do not say anything other than the sentence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here is a list of words followed by their proficiency levels in the scale of 0 to 1.\n",
      "0 being the lowest proficiency level and 1 being the highest.\n",
      "\n",
      "di: 0.6, mezzi: 0.6, camicia: 0.6, casa: 0.6, l'aspetto: 0.6, zia: 0.6, quantità: 0.6, barba: 0.6, bottiglia: 0.6, capelli: 0.6, le: 0.6, capodanno: 0.6, berretto: 0.6, maglia: 0.6, famiglia: 0.6, l'autobus: 0.6, fratello: 0.6, sorella: 0.6, il: 0.6, cugina: 0.6, cognato: 0.6, tonno: 0.6, madre: 0.6, nipote: 0.6, pesce: 0.6, guanti: 0.6, vecchio: 0.6, nero: 0.6, lunghi: 0.6, l'uovo: 0.6, nuora: 0.6, riso: 0.6, azzurro: 0.6, gli: 0.6, alto: 0.6, genitori: 0.6, fragola: 0.6, ondulati: 0.6, contenitori: 0.6, stivali: 0.6, l'aglio: 0.6, fratelli: 0.6, nonni: 0.6, marito: 0.6, divertente: 0.6, marrone: 0.6, cappotto: 0.6, lisci: 0.6, cravatta: 0.6, suoceri: 0.6, serio: 0.6, alimenti: 0.6, yogurt: 0.6, marmellata: 0.6, allegro: 0.6, lentiggini: 0.6, matrimonio: 0.6, fontana: 0.6, pollo: 0.6, figlio: 0.6, foglia: 0.6, scarpe: 0.6, zio: 0.6, rosso: 0.6, blu: 0.6, torta: 0.6, sacchetto: 0.6, giallo: 0.6, figli: 0.6, birra: 0.6, e: 0.6, completo: 0.6, colori: 0.6, treno: 0.6, metropolitana: 0.6, moglie: 0.6, maglietta: 0.6, patata: 0.6, magro: 0.6, giovane: 0.6, natura: 0.6, limone: 0.6, l'albero: 0.6, borsa: 0.6, giacca: 0.6, nuvola: 0.6, salsiccia: 0.6, l'edificio: 0.6, denti: 0.6, fetta: 0.6, bicicletta: 0.6, l'anniversario: 0.6, strada: 0.6, i: 0.6, patatine: 0.6, maglione: 0.6, prato: 0.6, d'acqua: 0.6, piedi: 0.6, gonna: 0.6, pantaloni: 0.6, l'arancia: 0.6, ginnastica: 0.6, vestito: 0.6, antipatico: 0.6, cetriolo: 0.6, occhi: 0.6, compleanno: 0.6, grigio: 0.6, nipoti: 0.6, foulard: 0.6, simpatico: 0.6, l'insalata: 0.6, nonna: 0.6, metro: 0.6, sandali: 0.6, l'aereo: 0.6, chiesa: 0.6, padre: 0.6, pacco: 0.6, sciarpa: 0.6, lo: 0.6, piccolo: 0.6, fiume: 0.6, suocera: 0.6, da: 0.6, jeans: 0.6, mucca: 0.6, calvo: 0.6, fiore: 0.6, l'auto: 0.6, ricci: 0.6, tram: 0.6, mela: 0.6, nave: 0.6, cugini: 0.6, festività: 0.6, nonno: 0.6, zii: 0.6, l'abito: 0.6, barattolo: 0.6, natale: 0.6, l'ape: 0.6, bocca: 0.6, a: 0.6, scatoletta: 0.6, noioso: 0.6, cipolla: 0.6, piccione: 0.6, arancione: 0.6, genero: 0.6, naso: 0.6, portare: 0.6, carne: 0.6, camicetta: 0.6, panchina: 0.6, pane: 0.6, trasportola: 0.6, orecchie: 0.6, corti: 0.6, pasta: 0.6, occhiali: 0.6, latte: 0.6, bianco: 0.6, figlia: 0.6, cielo: 0.6, verde: 0.6, grasso: 0.6, macchina: 0.6, mossi: 0.6, moto: 0.6, vestiti: 0.6, cugino: 0.6, basso: 0.6, suocero: 0.6, grande: 0.6, montagna: 0.6, sole: 0.6, la: 0.6, lattina: 0.6, avere: 0.6, \n",
      "\n",
      "Make italian sentences using some of the words in the list above. \n",
      "Try to prioritize words with a lower proficiency level while keeping the sentence natural.\n",
      "\n",
      "Do not say anything other than the sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(msg_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m   msg\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproficiency[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word))]\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(msg)\n\u001b[0;32m----> 7\u001b[0m msg_list\u001b[38;5;241m=\u001b[39m\u001b[43mload_vocab_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./vocab_datas/italian.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m msg_system\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSei un insegnante di lingue. L\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutente sta imparando nuove parole.\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      9\u001b[0m msg_user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mGenera una frase naturale e grammaticalmente corretta in italiano utilizzando alcune delle parole nell\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melenco sottostante.\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mCerca di dare la priorità alle parole con un livello di competenza più basso, mantenendo comunque la frase naturale.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mmsg_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mload_vocab_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_vocab_data\u001b[39m(file_path):\n\u001b[0;32m----> 2\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mloadtxt(file_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m      3\u001b[0m   word\u001b[38;5;241m=\u001b[39m data[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m   proficiency \u001b[38;5;241m=\u001b[39m data[:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def load_vocab_data(file_path):\n",
    "  data = np.loadtxt(file_path, delimiter=\",\", dtype=str)\n",
    "  word= data[:,0]\n",
    "  proficiency = data[:,1].astype(int)/5\n",
    "  msg=[f\"{word[i]}: {proficiency[i]}, \" for i in range(len(word))]\n",
    "  return ('').join(msg)\n",
    "msg_list=load_vocab_data(\"./vocab_data/italian.csv\")\n",
    "msg_system=\"Sei un insegnante di lingue. L'utente sta imparando nuove parole.\" \n",
    "msg_user=f\"\"\"\n",
    "Genera una frase naturale e grammaticalmente corretta in italiano utilizzando alcune delle parole nell'elenco sottostante.\n",
    "Cerca di dare la priorità alle parole con un livello di competenza più basso, mantenendo comunque la frase naturale.\n",
    "\n",
    "Ecco un elenco di parole seguite dal loro livello di competenza su una scala da 0 a 1,\n",
    "dove 0 rappresenta il livello di competenza più basso e 1 il più alto.\n",
    "\n",
    "{msg_list}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", device=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_system},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_user},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_system},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_user},]\n",
    "        },\n",
    "        {'role': 'assistant',\n",
    "         'content': 'Okay, that’s fantastic! I’m ready to help you build sentences using those words.'},\n",
    "        {'role': 'user',\n",
    "         'content': 'Now generate the sentence as instructed'},\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages,min_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_system},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": msg_user},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(messages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': [{'role': 'system',\n",
       "     'content': [{'type': 'text',\n",
       "       'text': 'You are a language tutor. The user is learning new words.'}]},\n",
       "    {'role': 'user',\n",
       "     'content': [{'type': 'text',\n",
       "       'text': \"\\nHere is a list of words followed by their proficiency levels in the scale of 0 to 1.\\n0 being the lowest proficiency level and 1 being the highest.\\n\\njeans: 0.6, l'insalata: 0.6, macchina: 0.6, lo: 0.6, fiore: 0.6, l'edificio: 0.6, blu: 0.6, ricci: 0.6, nonni: 0.6, compleanno: 0.6, pane: 0.6, gonna: 0.6, metro: 0.6, cravatta: 0.6, nero: 0.6, colori: 0.6, capodanno: 0.6, completo: 0.6, le: 0.6, limone: 0.6, nuvola: 0.6, fratello: 0.6, prato: 0.6, marito: 0.6, maglietta: 0.6, maglia: 0.6, bicicletta: 0.6, lunghi: 0.6, l'albero: 0.6, zii: 0.6, antipatico: 0.6, barattolo: 0.6, bianco: 0.6, festività: 0.6, cugino: 0.6, suoceri: 0.6, allegro: 0.6, carne: 0.6, tonno: 0.6, magro: 0.6, l'ape: 0.6, patata: 0.6, di: 0.6, rosso: 0.6, grande: 0.6, mela: 0.6, pasta: 0.6, barba: 0.6, avere: 0.6, moto: 0.6, marrone: 0.6, stivali: 0.6, gli: 0.6, l'abito: 0.6, nipoti: 0.6, moglie: 0.6, zio: 0.6, montagna: 0.6, l'aereo: 0.6, piccione: 0.6, la: 0.6, marmellata: 0.6, e: 0.6, lisci: 0.6, alto: 0.6, mezzi: 0.6, sandali: 0.6, matrimonio: 0.6, natale: 0.6, fontana: 0.6, fiume: 0.6, d'acqua: 0.6, camicia: 0.6, zia: 0.6, l'auto: 0.6, salsiccia: 0.6, birra: 0.6, nave: 0.6, ginnastica: 0.6, giacca: 0.6, scarpe: 0.6, cognato: 0.6, contenitori: 0.6, foglia: 0.6, portare: 0.6, basso: 0.6, pantaloni: 0.6, strada: 0.6, guanti: 0.6, torta: 0.6, alimenti: 0.6, denti: 0.6, lentiggini: 0.6, famiglia: 0.6, trasportola: 0.6, vecchio: 0.6, l'anniversario: 0.6, cugini: 0.6, ondulati: 0.6, sciarpa: 0.6, verde: 0.6, camicetta: 0.6, grasso: 0.6, scatoletta: 0.6, figli: 0.6, yogurt: 0.6, bocca: 0.6, l'aglio: 0.6, nuora: 0.6, fetta: 0.6, foulard: 0.6, bottiglia: 0.6, corti: 0.6, suocero: 0.6, figlio: 0.6, lattina: 0.6, vestito: 0.6, vestiti: 0.6, calvo: 0.6, azzurro: 0.6, figlia: 0.6, divertente: 0.6, madre: 0.6, giallo: 0.6, pacco: 0.6, nonno: 0.6, orecchie: 0.6, l'arancia: 0.6, cetriolo: 0.6, arancione: 0.6, l'uovo: 0.6, noioso: 0.6, berretto: 0.6, patatine: 0.6, cielo: 0.6, serio: 0.6, metropolitana: 0.6, giovane: 0.6, sacchetto: 0.6, suocera: 0.6, riso: 0.6, chiesa: 0.6, borsa: 0.6, sole: 0.6, mossi: 0.6, genitori: 0.6, nonna: 0.6, grigio: 0.6, maglione: 0.6, cugina: 0.6, occhiali: 0.6, piedi: 0.6, occhi: 0.6, capelli: 0.6, padre: 0.6, natura: 0.6, panchina: 0.6, piccolo: 0.6, fratelli: 0.6, da: 0.6, quantità: 0.6, cipolla: 0.6, naso: 0.6, pollo: 0.6, il: 0.6, pesce: 0.6, genero: 0.6, latte: 0.6, l'aspetto: 0.6, casa: 0.6, simpatico: 0.6, mucca: 0.6, l'autobus: 0.6, tram: 0.6, a: 0.6, i: 0.6, fragola: 0.6, sorella: 0.6, treno: 0.6, nipote: 0.6, cappotto: 0.6, \\n\\nMake italian sentences using some of the words in the list above. \\nTry to prioritize words with a lower proficiency level while keeping the sentence natural.\\n\\nDo not say anything other than the sentence.\\n\"}]},\n",
       "    {'role': 'assistant',\n",
       "     'content': 'Il nonno ha i capelli bianchi e una barba grande.'}]}]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'I go. The cat runs. I eat. The dog is big. I read a book with a red pen. The sun is big. \\nTranslation: I go. The cat runs. I eat. The dog is big. I read a book with a red pen. The sun is big.\\nExplanation: These sentences use simple present tense and basic sentence structure (Subject-Verb-Object/Complement). \"I\" is the subject in many sentences, practicing personal pronouns. Articles (\"a\", \"the\") are used appropriately for introducing nouns. Adjectives (“big”, “red”) modify nouns. The verb \"to be\" is used to describe characteristics (\"is big\"). Sentence structure remains simple and clear for A1 learners.\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Sentences: I go. The cat runs. I eat. The dog is big. I read a book with a red pen. The sun is big. \\nTranslation: I go. The cat runs. I eat. The dog is big. I read a book with a red pen. The sun is big.\\nExplanation: These sentences use simple present tense and basic sentence structure (Subject-Verb-Object/Complement). \\\"I\\\" is the subject in many sentences, practicing personal pronouns. Articles (\\\"a\\\", \\\"the\\\") are used appropriately for introducing nouns. Adjectives (“big”, “red”) modify nouns. The verb \\\"to be\\\" is used to describe characteristics (\\\"is big\\\"). Sentence structure remains simple and clear for A1 learners.\\n\\n\\n\\n\"\n",
    "text.split('Sentences: ')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
